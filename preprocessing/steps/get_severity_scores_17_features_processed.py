from __future__ import print_function

import psycopg2
import datetime
import sys
from operator import itemgetter, attrgetter, methodcaller
import numpy as np
import itertools
import os.path
import matplotlib.pyplot as plt
import math
from multiprocessing import Pool, cpu_count
import re
import traceback
import shutil
from pathlib import Path
from sklearn.linear_model import LogisticRegression
from tqdm import tqdm

from preprocessing.utils import getConnection
from preprocessing.utils import parseUnitsMap
from preprocessing.utils import parseNum
from preprocessing.utils import sparsify


# # How to use code in this folder
#
# This folder generates materialized views for a variety of severity of illness scores (detailed below).
# The queries make use of materialized views which aggregate data from the first day of a patient's ICU stay. To run the code, it is necessary to:
#
# 1. Run the scripts which generate the constituent materialized views
#     * echodata - generated by /concepts/echo-data.sql
#     * ventdurations - generated by /concepts/ventilation-durations.sql - (needed for subsequent view)
#     * vitalsfirstday - generated by /concepts/firstday/vitals-first-day.sql
#     * uofirstday - generated by /concepts/firstday/urine-output-first-day.sql
#     * ventfirstday - generated by /concepts/firstday/ventilated-first-day.sql
#     * gcsfirstday - generated by /concepts/firstday/gcs-first-day.sql
#     * labsfirstday - generated by /concepts/firstday/labs-first-day.sql
#     * bloodgasfirstday - generated by /concepts/firstday/blood-gas-first-day.sql - (needed for subsequent view)
#     * bloodgasfirstdayarterial - generated by /concepts/firstday/blood-gas-first-day-arterial.sql
# 2. Run the script for the severity of illness score you are interested in
#     * OASIS - oasis.sql
#     * SAPS - saps.sql
#     * SOFA - sofa.sql
#
# The scripts were written and tested in PostgreSQL 9.4.4. The scripts are written to follow the SQL standard as close as possible, and there should be minimal changes necessary for them to operate under different languages. Key changes likely involve:
#   * Changing "with" clause to subqueries if using MySQL
#   * Modifying the regular expression in the *echo* view in sofa.sql as appropriate
#
# If you modify a script to operate in another RDBMS, we would welcome a pull request!
#

# In[2]:

def get_severity_scores_17_features_processed_hrs(args, hrs):
    HRS = hrs
    hrs_suffix_dict = {
        24: '', 48: '_48'
    }
    hrs_suffix = hrs_suffix_dict[HRS]
    cachedir = Path(args.cachedir)
    conn = getConnection()
    cur = conn.cursor()
    working_dir = './mimic-code/'

    # prepare necessary materialized views

    # sqlfilelist = [
    #     'concepts/echo-data.sql',
    #     'concepts/ventilation-durations.sql',
    #     'concepts/firstday/vitals-first-day.sql',
    #     'concepts/firstday/urine-output-first-day.sql',
    #     'concepts/firstday/ventilation-first-day.sql',
    #     'concepts/firstday/gcs-first-day.sql',
    #     'concepts/firstday/labs-first-day.sql',
    #     'concepts/firstday/blood-gas-first-day.sql',
    #     'concepts/firstday/blood-gas-first-day-arterial.sql'
    # ]

    # for sqlfile in sqlfilelist:
    #     pstr = os.path.join(working_dir, sqlfile)
    #     if not os.path.exists(pstr):
    #         print(pstr)

    # for sqlfile in sqlfilelist:
    #     print('executing {0}...'.format(sqlfile))
    #     with open(os.path.join(working_dir, sqlfile), 'r') as f:
    #         sql = f.read()
    #         cur.execute(sql)
    #         conn.commit()
    #     print('finish executing {0}!'.format(sqlfile))

    conn = getConnection()
    cur = conn.cursor()
    # sapsii
    with open(os.path.join(working_dir, 'concepts{}/severityscores/sapsii.sql'.format(hrs_suffix)), 'r') as f:
        cur.execute(f.read())
        conn.commit()

    # sofa
    with open(os.path.join(working_dir, 'concepts{}/severityscores/sofa.sql'.format(hrs_suffix)), 'r') as f:
        cur.execute(f.read())
        conn.commit()

    # create indices
    conn = getConnection()
    cur = conn.cursor()
    for viewname in ['SAPS{}'.format(hrs_suffix), 'SOFA{}'.format(hrs_suffix)]:
        comm = 'DROP INDEX IF EXISTS {0}_hadm_id_idx; CREATE INDEX IF NOT EXISTS {0}_hadm_id_idx ON mimiciii.{0} (hadm_id);'.format(viewname)
        cur.execute(comm)
        conn.commit()

    # In[3]:

    TARGETDIR = cachedir.joinpath('admdata_17f')
    HRDIR = os.path.join(TARGETDIR, '%dhrs' % HRS)
    RESDIR = os.path.join(HRDIR, 'non_series')

    data_all = np.load(os.path.join(
        HRDIR, 'DB_merged_%dhrs.npy' % HRS), allow_pickle=True).tolist()
    valid_aids = [t[0][-1] for t in data_all]
    print(len(valid_aids))
    print(valid_aids)

    # In[6]:

    # get sapsii scores and sofa scores

    conn = getConnection()
    cur = conn.cursor()

    # for sapsii scores, we have the sapsii and sapsii_prob
    def get_sapsii(aid):
        conn = getConnection()
        sql = 'select * from mimiciii.sapsii{} where hadm_id = {}'.format(
            hrs_suffix, aid)
        cur = conn.cursor()
        cur.execute(sql)
        res = cur.fetchone()
        if res:
            return res
        else:
            return None

    # for sofa scores, we only have the score and we need lr to process
    def get_sofa_score(aid):
        conn = getConnection()
        sql = 'select sofa from mimiciii.sofa{} where hadm_id = {}'.format(
            hrs_suffix, aid)
        cur = conn.cursor()
        cur.execute(sql)
        res = cur.fetchone()
        if res:
            return res[0]
        else:
            return None

    print('sapsii...')
    # store sapsii subscores in file
    # p = Pool(args.num_workers)
    # ress = [p.apply_async(get_sapsii, args=(aid,)) for aid in valid_aids]
    # p.close()
    # p.join()
    # ress = np.array([x.get() for x in ress])
    ress = []
    for aid in tqdm(valid_aids):
        ress.append(get_sapsii(aid))
    print(ress[:10])
    np.savez_compressed(os.path.join(RESDIR, 'sapsii.npz'), sapsii=ress)

    print('sofa...')
    # store sofa scores in file
    # p = Pool(args.num_workers)
    # ress = [p.apply_async(get_sofa_score, args=(aid,)) for aid in valid_aids]
    # p.close()
    # p.join()
    # ress = np.array([x.get() for x in ress])
    ress = []
    for aid in tqdm(valid_aids):
        ress.append(get_sofa_score(aid))
    print(ress[:10])
    np.savez_compressed(os.path.join(RESDIR, 'sofa.npz'), sofa=ress)

    # In[8]:

    len(valid_aids)

    # In[ ]:

    # get sofa score: do logistic regression on sofa score
    label_mor = np.load(os.path.join(
        HRDIR, 'ADM_LABELS_%dhrs.npy' % HRS), allow_pickle=True).tolist()
    label_mor = [l[0] for l in label_mor]

    lr = LogisticRegression()
    X = np.array(np.load(os.path.join(RESDIR, 'sofa.npz'), allow_pickle=True)
                 ['sofa']).reshape(-1, 1)
    y = np.array(label_mor)
    lr.fit(X, y)
    score = lr.score(X, y)
    prob = lr.predict_proba(X)
    print(score)
    print(lr.classes_, prob)
    sofa_score = [p[1] for p in prob]

    np.savez_compressed(os.path.join(
        RESDIR, 'sofa_res.npz'), sofa_score=sofa_score)


def get_severity_scores_17_features_processed(args):
    get_severity_scores_17_features_processed_hrs(args, 24)
    get_severity_scores_17_features_processed_hrs(args, 48)